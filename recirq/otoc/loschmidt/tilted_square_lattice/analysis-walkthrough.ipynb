{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loschmidt Analysis Walkthrough\n",
    "\n",
    "This notebook walks through the analysis routines available for the `recirq.otoc.loschmidt.tilted_sqare_lattice` analysis routines. In particular, you will be guided on how to group, slice, fit, and plot data to extract fidelities from loschmidt echo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set up reasonable defaults for figure fonts\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update(**{\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'legend.title_fontsize': 12,\n",
    "    'figure.figsize': (7, 5),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Results\n",
    "\n",
    "This notebook assumes you have executed the `run-simulator.py` script in this directory to execute the algorithmic benchmark on a simulator, and that the results were saved with run_id `\"simulated-1\"`. Otherwise, please modify the `run_id` parameter below.\n",
    "\n",
    "After some standard imports, we load the `ExecutableGroupResult` in its entirety using `cirq_google.workflow` tools. This is the complete raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cirq_google as cg\n",
    "\n",
    "import recirq.otoc.loschmidt.tilted_square_lattice.analysis as analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results = cg.ExecutableGroupResultFilesystemRecord.from_json(run_id='simulated-1').load()\n",
    "repr(raw_results)[:100] + ' ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pandas for slicing and dicing\n",
    "\n",
    "We can extract the most relevant parameters from the raw results and flatten it into a tabular format. In particular, we use `pd.DataFrame` for further data aggregation and plotting.\n",
    "\n",
    "The measured `success_probability` ranges between 0 and 1, indicating the fraction of times the measured result after the echo actually returned to its initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.loschmidt_results_to_dataframe(raw_results)\n",
    "print(len(df), 'rows')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "Instead of considering each result in isolation, we can aggregate quantities to make more meaningful plots and fits.\n",
    "\n",
    "In the following, we use pandas group-by functionality to\n",
    " 1. Average (and compute the standard deviation) over random circuit instances, holding all else constant.\n",
    " 2. Plot these averaged quantities vs. macrocycle_depth, holding all else constant.\n",
    " \n",
    "We use `analysis.groupby_all_except` as a wrapper around `pd.DataFrame.groupby` so we can specify what we _don't_ want to aggregate over; which should make these analysis routines more extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import WHD_GB_COLS\n",
    "WHD_GB_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_y_cols = {\n",
    "    'success_probability_mean': ('success_probability', 'mean'),\n",
    "    'success_probability_std': ('success_probability', 'std'),\n",
    "    'job_finished_time': ('job_finished_time', 'last'),\n",
    "}\n",
    "means_df = df.groupby(WHD_GB_COLS).agg(**means_y_cols)\n",
    "means_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) vs. macrocycle_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import WH_GB_COLS\n",
    "WH_GB_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_depth_y_cols = {\n",
    "    'macrocycle_depth': list,\n",
    "    'success_probability_mean': list,\n",
    "    'success_probability_std': list,\n",
    "    'job_finished_time': 'last',\n",
    "}\n",
    "\n",
    "vs_depth_df = means_df.reset_index(level='macrocycle_depth').groupby(WH_GB_COLS).agg(vs_depth_y_cols)\n",
    "vs_depth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in vs_depth_df.reset_index().iterrows():\n",
    "    plt.errorbar(\n",
    "        x=row['macrocycle_depth'],\n",
    "        y=row['success_probability_mean'],\n",
    "        yerr=row['success_probability_std'],\n",
    "        label=', '.join(f'{row[col]}' for col in WH_GB_COLS),\n",
    "        capsize=5, ls='', marker='o',\n",
    "    )\n",
    "    \n",
    "plt.xlabel('Macrocycle Depth')\n",
    "plt.ylabel('Success Probability')\n",
    "plt.legend(title=','.join(WH_GB_COLS), loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two aggregation steps (1) and (2) are encapsulated in `analysis.agg_vs_macrocycle_depth`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "\n",
    "Part of the loschmidt echo analysis involves fitting an exponential decay in success probability vs. macrocycle_depth to robustly estimate a per-macrocycle error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape data for fitting\n",
    "\n",
    "Now, we group all `(macrocycle_depth, instance_i)` points into a list holding everything else constant so we can use the raw points for fitting. We'll get 3 fits (one for each row) with the example dataframe in this notebook. The following cell shows that groupby operation used under the hood in `analysis.fit_vs_macrocycle_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_size_y_cols = {\n",
    "    'macrocycle_depth': list,\n",
    "    'success_probability': list,\n",
    "    'job_finished_time': 'last',\n",
    "    'n_qubits': analysis.assert_one_unique_val,\n",
    "}\n",
    "vs_size_df = df.groupby(WH_GB_COLS).agg(vs_size_y_cols)\n",
    "vs_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df, exp_ansatz = analysis.fit_vs_macrocycle_depth(df)\n",
    "fit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Plotting\n",
    "\n",
    "To plot the mean+stddev data as well as visualizations of the fits, we merge (join) the two dataframes. Note that the groupby columns of the two dataframes are the same, so we can join on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_cols = ['job_finished_time']\n",
    "total_df = fit_df.join(vs_depth_df.drop(duplicate_cols, axis=1))\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.get_cmap('tab10')\n",
    "\n",
    "for i, row in total_df.reset_index().iterrows():\n",
    "    plt.errorbar(\n",
    "        x=row['macrocycle_depth'],\n",
    "        y=row['success_probability_mean'],\n",
    "        yerr=row['success_probability_std'],\n",
    "        marker='o', capsize=5, ls='',\n",
    "        color=colors(i),\n",
    "        label=f'{row[\"width\"]}x{row[\"height\"]} ({row[\"n_qubits\"]}q) {row[\"processor_str\"]}; f={row[\"f\"]:.3f}'\n",
    "    )\n",
    "    \n",
    "    xx = np.linspace(np.min(row['macrocycle_depth']), np.max(row['macrocycle_depth']))\n",
    "    yy = exp_ansatz(xx, a=row['a'], f=row['f'])\n",
    "    plt.plot(xx, yy, ls='--', color=colors(i))\n",
    "    \n",
    "plt.legend(loc='best')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Macrocycle Depth')\n",
    "plt.ylabel('Success Probability')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit vs. \"Quantum Area\"\n",
    "\n",
    "In a local depolarizing model, we expect success to decay exponentially in circuit depth and the number of qubits. We define a quantity called quantum area (`q_area`) which is the circuit width (i.e. number of qubits) multiplied by its depth. This is the number of operations in the circuit (also including any idle operations).\n",
    "\n",
    "By defining this new quantity, we can fit a curve of fidelity vs. quantum area. The following cell shows the groupby operation used in `analysis.fit_vs_q_area`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import BASE_GB_COLS\n",
    "BASE_GB_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_q_area_y_cols = {\n",
    "    'q_area': list,\n",
    "    'success_probability': list,\n",
    "    'job_finished_time': 'last',\n",
    "}\n",
    "df.groupby(BASE_GB_COLS).agg(vs_q_area_y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df2, exp_ansatz_vs_q_area = analysis.fit_vs_q_area(df)\n",
    "fit_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll merge (join) the raw data with the fits data. We use `analysis.agg_vs_q_area` to aggregate the raw data. It works very similar to the `vs_depth` aggregation explained above.\n",
    "\n",
    "Using dataframes at this level of aggregation seems like overkill since everything has been grouped into one DataFrame row, but this code will run without modification when comparing multiple runs or multiple processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_q_area_df, vs_q_area_gb_cols = analysis.agg_vs_q_area(df)\n",
    "\n",
    "duplicate_cols = ['job_finished_time']\n",
    "total_df2 = fit_df2.join(vs_q_area_df.drop(duplicate_cols, axis=1))\n",
    "total_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.get_cmap('tab10')\n",
    "\n",
    "for i, row in total_df2.reset_index().iterrows():\n",
    "    plt.errorbar(x=row['q_area'], \n",
    "                 y=row['success_probability_mean'], \n",
    "                 yerr=row['success_probability_std'],\n",
    "                 color=colors(i), capsize=5, marker='o', ls='')\n",
    "    xx = np.linspace(np.min(row['q_area']), np.max(row['q_area']))\n",
    "    yy = exp_ansatz_vs_q_area(xx, a=row['a'], f=row['f'])\n",
    "    plt.plot(xx, yy, ls='--', color=colors(i),\n",
    "             label=f'{row[\"run_id\"]}; f={row[\"f\"]:.3f}')\n",
    "\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Quantum Area')\n",
    "plt.ylabel('Success Probability')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
